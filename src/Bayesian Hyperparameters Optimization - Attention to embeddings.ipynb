{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import dataset\n",
    "import pickle\n",
    "from dataset import DataSet\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to optimize:\n",
    "- Number of hidden Neurons\n",
    "- Init stdev\n",
    "- Learning rate\n",
    "- optimizer\n",
    "- rnn_type\n",
    "- batch_size\n",
    "- embedding_size\n",
    "- dropout\n",
    "- l2\n",
    "- layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_model_base = 'BO_Attention_RNN_Adam_'\n",
    "num_model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some auxilar functions\n",
    "def _seq_length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def _last_relevant(output, length):\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    out_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(parameters):\n",
    "    \n",
    "    print(parameters)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    # Define placeholders\n",
    "    x = tf.placeholder(\"float\", [None, parameters['seq_length'], parameters['n_input']], name='x')\n",
    "    y = tf.placeholder(\"float\", [None, parameters['n_output']], name='y')\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "    # Define weights and bias - For now we will try with attention to hidden state \n",
    "    weights = {\n",
    "        'alphas': tf.Variable(tf.random_normal([parameters['n_hidden'], 1], stddev=parameters['init_stdev']), name='w_alphas'),\n",
    "        'out': tf.Variable(tf.random_normal([parameters['embedding_size'], parameters['n_output']], stddev=parameters['init_stdev']), name='w_out'),\n",
    "        'emb': tf.Variable(tf.random_normal([parameters['n_input'], parameters['embedding_size']], stddev=parameters['init_stdev']), name='w_emb')\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([parameters['n_output']]), name='b_out'),\n",
    "        'alphas': tf.Variable(tf.random_normal([1]), name='b_alphas'),\n",
    "        'emb': tf.Variable(tf.random_normal([parameters['embedding_size']]), name='b_emb')\n",
    "    }\n",
    "\n",
    "    # Compute embeddings\n",
    "    x_reshaped = tf.reshape(x, [-1, int(x.get_shape()[2])])\n",
    "    if parameters['embedding_activation'] == 'linear':\n",
    "        v = tf.matmul(x_reshaped, weights['emb'])\n",
    "    elif parameters['embedding_activation'] == 'tanh':\n",
    "        v = tf.tanh(tf.matmul(x_reshaped, weights['emb']) + biases['emb'])\n",
    "    elif parameters['embedding_activation'] == 'sigmoid':\n",
    "        v = tf.sigmoid(tf.matmul(x_reshaped, weights['emb']) + biases['emb'])\n",
    "    v_reshaped = tf.reshape(v, [-1, parameters['seq_length'], parameters['embedding_size']])\n",
    "    if parameters['layer_norm']:\n",
    "        v_reshaped = tf.contrib.layers.layer_norm(v_reshaped)\n",
    "\n",
    "    # Define RNN\n",
    "    if parameters['rnn_type'].lower() == 'lstm':\n",
    "        rnn_cell = tf.contrib.rnn.BasicLSTMCell(parameters['n_hidden'], forget_bias=1.0)\n",
    "    elif parameters['rnn_type'] == 'lstm2':\n",
    "        rnn_cell = tf.contrib.rnn.LSTMCell(parameters['n_hidden'])\n",
    "    elif parameters['rnn_type'].lower() == 'gru':\n",
    "        rnn_cell = tf.contrib.rnn.GRUCell(parameters['n_hidden'])\n",
    "    elif parameters['rnn_type'] == 'rnn':\n",
    "        rnn_cell = tf.contrib.rnn.BasicRNNCell(parameters['n_hidden'])\n",
    "    elif parameters['rnn_type'] == 'lstm_normalized':\n",
    "        rnn_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(parameters['n_hidden'])\n",
    "    #Add dropout\n",
    "    if parameters['dropout'] > 0:\n",
    "        rnn_cell = tf.contrib.rnn.DropoutWrapper(rnn_cell, output_keep_prob=dropout_keep_prob)\n",
    "    outputs, states = tf.nn.dynamic_rnn(\n",
    "        rnn_cell,\n",
    "        v_reshaped,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=_seq_length(v_reshaped)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Define attention weihts\n",
    "    outputs_reshaped = tf.reshape(outputs, [-1, int(outputs.get_shape()[2])])\n",
    "    ejs = tf.matmul(outputs_reshaped, weights['alphas']) + biases['alphas'] \n",
    "    ejs_reshaped = tf.reshape(ejs, [-1, int(outputs.get_shape()[1])])\n",
    "    alphas = tf.nn.softmax(ejs_reshaped, name='attention_weights') \n",
    "    reshaped_alphas = tf.reshape(alphas, [-1, 1])\n",
    "    # Define context\n",
    "    context = reshaped_alphas * v\n",
    "    context_reshaped = tf.reshape(context, [-1, parameters['seq_length'], int(context.get_shape()[1])])\n",
    "    context_reduced = tf.reduce_sum(context_reshaped, axis= 1)\n",
    "\n",
    "    # Normalize context by number of timesteps?\n",
    "    # Define logits and loss\n",
    "    logits = tf.matmul(context_reduced, weights['out']) + biases['out']\n",
    "    #pred_prob = tf.nn.softmax(logits, name=\"predictions\") # SIGMOID!!!!!!!!\n",
    "    pred_prob = tf.sigmoid(logits, name=\"predictions\")\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) SIGMOID!!!\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    # L2 regularization\n",
    "    for var in tf.trainable_variables():\n",
    "        if ('b_out' not in var.name) and ('b_alphas' not in var.name) and ('b_emb' not in var.name) and ('bias' not in var.name) and ('LayerNorm' not in var.name):\n",
    "            print('Variable ' + var.name + ' will be regularized')\n",
    "            loss += parameters['l2'] * tf.nn.l2_loss(var)\n",
    "\n",
    "\n",
    "    #Define optimizer\n",
    "    if parameters['optimizer'].lower() == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=parameters['learning_rate']).minimize(loss)\n",
    "    elif parameters['optimizer'].lower() == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=parameters['learning_rate']).minimize(loss)\n",
    "    elif parameters['optimizer'].lower() == 'adadelta':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(learning_rate=parameters['learning_rate']).minimize(loss)\n",
    "    elif parameters['optimizer'].lower() == 'adagrad':\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=parameters['learning_rate']).minimize(loss)\n",
    "\n",
    "    # Initialization\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #Add summaries\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Create summaries to visualize weights\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.name, var)\n",
    "    # Summarize all gradients\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.name + '/gradient', grad)\n",
    "            \n",
    "    return x,y,dropout_keep_prob,loss,init,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open(\"preprocessed/dataset_augmented.pickle\", 'rb') as handle:\n",
    "        dataset = pickle.load(handle)\n",
    "    ds = DataSet(dataset)\n",
    "    del dataset\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(name_model, parameters, x, y, dropout_keep_prob, loss, init, optimizer, ds):\n",
    "    path_export_model = \"protobuf_models/\" + name_model + \"/\"\n",
    "    display_train_loss = 200\n",
    "    steps_periodic_checkpoint = 200\n",
    "    current_epoch = 0\n",
    "\n",
    "    # Start training\n",
    "    saver_last = tf.train.Saver()\n",
    "    saver_best = tf.train.Saver()\n",
    "    checkpoint_dir = './checkpoints/' + name_model + '/'\n",
    "    if not tf.gfile.Exists(checkpoint_dir):\n",
    "        tf.gfile.MakeDirs(checkpoint_dir)\n",
    "        tf.gfile.MakeDirs(checkpoint_dir + '/best_model')\n",
    "        tf.gfile.MakeDirs(checkpoint_dir + '/last_model')\n",
    "    best_loss = 150000000\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Create FileWriters for summaries\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('tensorboard/' + name_model + '/train', sess.graph)\n",
    "        val_writer = tf.summary.FileWriter('tensorboard/' + name_model + '/val', sess.graph)\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        step = 1\n",
    "        while ds.get_current_epoch('train') < parameters['num_epochs']:\n",
    "\n",
    "            # Get next batch\n",
    "            batch_x, batch_y = ds.next_batch(parameters['batch_size'])\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            _ = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, dropout_keep_prob: (1 - parameters['dropout'])})\n",
    "\n",
    "            # Compute train loss\n",
    "            if step % display_train_loss == 0 or step == 1:\n",
    "                # Calculate batch loss\n",
    "                train_loss, summary = sess.run([loss, merged], feed_dict={x: batch_x, y: batch_y, dropout_keep_prob: 1})\n",
    "                print(\"Step \" + str(step) + \", Train Loss: \" + str(train_loss))\n",
    "                train_writer.add_summary(summary, step * parameters['batch_size'])\n",
    "\n",
    "\n",
    "            # Periodic model checkpoint\n",
    "            if step % steps_periodic_checkpoint == 0:\n",
    "                checkpoint_dir_tmp = checkpoint_dir + '/last_model/'\n",
    "                checkpoint_path = os.path.join(checkpoint_dir_tmp, 'model_last.ckpt')\n",
    "                saver_last.save(sess, checkpoint_path, global_step=step*parameters['batch_size'])\n",
    "\n",
    "            # Compute val loss and save model at the end of each epoch\n",
    "            if ds.get_current_epoch('train') != current_epoch:\n",
    "                current_epoch = ds.get_current_epoch('train')\n",
    "                X_val, Y_val = ds.get_set('val')\n",
    "                val_loss, summary = sess.run([loss, merged], feed_dict={x: batch_x, y: batch_y, dropout_keep_prob: 1})\n",
    "                print(\"----End epoch \" + str(current_epoch - 1) + \", Val Loss: \" + str(val_loss))\n",
    "                val_writer.add_summary(summary, step * parameters['batch_size'])\n",
    "\n",
    "                # Check if validation loss is better\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    checkpoint_dir_tmp = checkpoint_dir + '/best_model/'\n",
    "                    checkpoint_path = os.path.join(checkpoint_dir_tmp, 'model_best.ckpt')\n",
    "                    saver_best.save(sess, checkpoint_path, global_step=step*parameters['batch_size'])\n",
    "\n",
    "\n",
    "                # Saved Model Builder \n",
    "                export_path = path_export_model + \"epoch\" + str(current_epoch - 1)\n",
    "                builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "                builder.add_meta_graph_and_variables(\n",
    "                      sess, [tf.saved_model.tag_constants.SERVING])\n",
    "                builder.save()\n",
    "                \n",
    "                if (current_epoch > 1) and (val_loss > 2):\n",
    "                    return best_loss\n",
    "                \n",
    "                if (current_epoch > 4) and (val_loss > 0.5):\n",
    "                    return best_loss\n",
    "                \n",
    "                if (current_epoch > 8) and (val_loss > 0.3):\n",
    "                    return best_loss\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "    return best_loss\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_evaluate(n_hidden,\n",
    "                 init_stdev,\n",
    "                 learning_rate,\n",
    "                 optimizer,\n",
    "                 rnn_type,\n",
    "                 batch_size,\n",
    "                  embedding_size,\n",
    "                  dropout,\n",
    "                  l2,\n",
    "                  layer_norm):\n",
    "\n",
    "    global num_model\n",
    "    name_model = name_model_base + str(num_model)\n",
    "    num_model += 1\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['seq_length'] = 18\n",
    "    parameters['n_input'] = 48\n",
    "    parameters['n_output'] = 24\n",
    "    parameters['n_hidden'] = int(n_hidden)\n",
    "    parameters['init_stdev'] = init_stdev\n",
    "    parameters['learning_rate'] = learning_rate\n",
    "    parameters['optimizer'] = 'adam' #optimizer\n",
    "    parameters['rnn_type'] = 'lstm2'#rnn_type\n",
    "    parameters['num_epochs'] = 20\n",
    "    parameters['batch_size'] = int(batch_size)\n",
    "    parameters['embedding_size'] = int(embedding_size)\n",
    "    parameters['embedding_activation'] = 'linear'\n",
    "    parameters['dropout'] = dropout\n",
    "    parameters['l2'] = l2\n",
    "    parameters['layer_norm'] = False#layer_norm # Seems to work with 0.001 L2\n",
    "    \n",
    "    x,y,dropout_keep_prob,loss,init,optimizer = create_model(parameters)\n",
    "    ds = load_dataset()\n",
    "    best_loss = train_model(name_model, parameters, x, y, dropout_keep_prob, loss, init, optimizer, ds)\n",
    "\n",
    "    return best_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   batch_size |   dropout |   embedding_size |   init_stdev |        l2 |   layer_norm |   learning_rate |   n_hidden |   optimizer |   rnn_type | \n",
      "{'seq_length': 18, 'num_epochs': 20, 'n_hidden': 161, 'rnn_type': 'lstm2', 'batch_size': 160, 'l2': 0.025953017115059741, 'dropout': 0.38585186887732315, 'optimizer': 'adam', 'n_input': 48, 'layer_norm': False, 'embedding_size': 10, 'n_output': 24, 'learning_rate': 0.32330985157473197, 'init_stdev': 0.91109663163097165, 'embedding_activation': 'linear'}\n",
      "Variable w_alphas:0 will be regularized\n",
      "Variable w_out:0 will be regularized\n",
      "Variable w_emb:0 will be regularized\n",
      "Variable rnn/lstm_cell/kernel:0 will be regularized\n",
      "INFO:tensorflow:Summary name w_alphas:0 is illegal; using w_alphas_0 instead.\n",
      "INFO:tensorflow:Summary name w_out:0 is illegal; using w_out_0 instead.\n",
      "INFO:tensorflow:Summary name w_emb:0 is illegal; using w_emb_0 instead.\n",
      "INFO:tensorflow:Summary name b_out:0 is illegal; using b_out_0 instead.\n",
      "INFO:tensorflow:Summary name b_alphas:0 is illegal; using b_alphas_0 instead.\n",
      "INFO:tensorflow:Summary name b_emb:0 is illegal; using b_emb_0 instead.\n",
      "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0 is illegal; using rnn/lstm_cell/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0 is illegal; using rnn/lstm_cell/bias_0 instead.\n",
      "INFO:tensorflow:Summary name w_alphas:0/gradient is illegal; using w_alphas_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_out:0/gradient is illegal; using w_out_0/gradient instead.\n",
      "INFO:tensorflow:Summary name w_emb:0/gradient is illegal; using w_emb_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b_out:0/gradient is illegal; using b_out_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b_alphas:0/gradient is illegal; using b_alphas_0/gradient instead.\n",
      "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0/gradient is illegal; using rnn/lstm_cell/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0/gradient is illegal; using rnn/lstm_cell/bias_0/gradient instead.\n"
     ]
    }
   ],
   "source": [
    "num_iter = 30\n",
    "init_points = 5\n",
    "\n",
    "\n",
    "rnnBO = BayesianOptimization(model_evaluate, {'n_hidden': (25, 250),\n",
    "                                              'init_stdev': (0.0001, 1),\n",
    "                                              'learning_rate': (0.0001, 1),\n",
    "                                              'optimizer': (0,1), # We could try to bin\n",
    "                                              #'optimizer': ['adam', 'sgd', 'adadelta', 'adagrad'],\n",
    "                                              'rnn_type': (0,1), # We could try to bin\n",
    "                                              #'rnn_type': ['lstm', 'lstm2', 'gru', 'lstm_normalized'],\n",
    "                                              'batch_size': (32,256),\n",
    "                                              #'batch_size': [32, 64, 128, 256],\n",
    "                                              'embedding_size': (8, 128),\n",
    "                                              'dropout': (0, 0.75),\n",
    "                                              'l2': (0, 0.1),\n",
    "                                              'layer_norm': (0,1) # We could try to bin\n",
    "                                              #'layer_norm': [False, True]\n",
    "                                              \n",
    "                                            })\n",
    "\n",
    "                                                            \n",
    "rnnBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "print('Final Results')\n",
    "print(rnnBO.res['max']['max_val'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "BO_Attention_RNN_2\n"
     ]
    }
   ],
   "source": [
    "variable_global = 3\n",
    "def test():\n",
    "    global num_model\n",
    "    print(num_model)\n",
    "    name_model = name_model_base + str(num_model)\n",
    "    print(name_model)\n",
    "    num_model += 1\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
